{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_k5ZDPYJtzh"
   },
   "source": [
    "### Setting up Environment and the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcoTVWytbVaV"
   },
   "source": [
    "#### Installing necessary packages\n",
    "First, we install Hugging Face and Llama Index libraries that would be used through out the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23832,
     "status": "ok",
     "timestamp": 1732141857061,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "S56dEc8gbMMP",
    "outputId": "2c2265ab-3bd3-41d4-910c-7d5321ac00e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index==0.10.19 in /usr/local/lib/python3.10/dist-packages (0.10.19)\n",
      "Requirement already satisfied: llama_index_core==0.10.19 in /usr/local/lib/python3.10/dist-packages (0.10.19)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (1.23.3)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.7)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.13)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.11)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.6)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.9.48.post4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.11)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.9)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.22)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.10.19) (0.1.6)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index_core==0.10.19) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (3.11.2)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (2024.9.0)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (0.27.2)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (3.4.2)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (1.54.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index_core==0.10.19) (0.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.26.2)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (3.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.46.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum) (3.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19) (4.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama_index_core==0.10.19) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index==0.10.19) (0.4.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama_index_core==0.10.19) (2.9.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index_core==0.10.19) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index_core==0.10.19) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index_core==0.10.19) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index_core==0.10.19) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index_core==0.10.19) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama_index_core==0.10.19) (0.14.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index_core==0.10.19) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index_core==0.10.19) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index_core==0.10.19) (2024.9.11)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index_core==0.10.19) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index_core==0.10.19) (0.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index_core==0.10.19) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index_core==0.10.19) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama_index_core==0.10.19) (3.1.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index_core==0.10.19) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama_index_core==0.10.19) (3.23.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index_core==0.10.19) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index_core==0.10.19) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index_core==0.10.19) (2024.2)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama_index_core==0.10.19) (1.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19) (2.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama_index_core==0.10.19) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama_index_core==0.10.19) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama_index_core==0.10.19) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
      "Requirement already satisfied: auto_gptq in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (1.1.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (3.1.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (0.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (1.26.4)\n",
      "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (1.0.1)\n",
      "Requirement already satisfied: gekko in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (1.2.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (2.5.1+cu121)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (0.4.5)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (4.46.2)\n",
      "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (0.13.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (4.66.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto_gptq) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto_gptq) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto_gptq) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto_gptq) (6.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->auto_gptq) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto_gptq) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto_gptq) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto_gptq) (0.20.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (3.11.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto_gptq) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto_gptq) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto_gptq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto_gptq) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto_gptq) (2024.2)\n",
      "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n"
     ]
    }
   ],
   "source": [
    "# install llama_index\n",
    "! pip install llama_index==0.10.19 llama_index_core==0.10.19 torch llama-index-embeddings-huggingface peft optimum bitsandbytes\n",
    "\n",
    "# install auto_gptq\n",
    "! pip install auto_gptq\n",
    "\n",
    "# install docx2txt\n",
    "! pip install docx2txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3cMZ8zKJtzj"
   },
   "source": [
    "#### LLM Setup\n",
    "Run the following cells below to import necessary libraries, setting up the Hugging Face LLM and prompting the LLM to get an response. In this mini project we use the Qwen2.5 LLM from Alibaba Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlJEueQjbSHo"
   },
   "outputs": [],
   "source": [
    "# import list of libraries\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings,SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsYssKyNdYoG"
   },
   "outputs": [],
   "source": [
    "# instantiate the LLM from the Hugging Face library\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\",\n",
    "                                             device_map=\"cuda:0\"\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtUV2T7JJtzl"
   },
   "source": [
    "###  Task 1 - LLM Prompting and Output Response Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAKvoYOwftnh"
   },
   "source": [
    "\n",
    "\n",
    "### Step 1: Prompt Generation\n",
    "\n",
    "In this step we write a funciton that can generate the prompts used for prompting the LLM. Populate the prompt generating functions below based on the following specs.\n",
    "\n",
    "**Input:** User query and context\n",
    "\n",
    "**Output:** Prompt as a string.\n",
    "\n",
    "**Example:**\n",
    "1. Context: A customer is having issues with their smartphone battery draining quickly and the phone overheating.\n",
    "2. User query: My battery is constantly draining, what are some suggestions.\n",
    "3. Example output prompt:\n",
    "```\n",
    "Context: A customer is having issues with their smartphone battery draining quickly and the phone overheating.\n",
    "Please respond to the following user comment. Use the context above if it is helpful.\n",
    "User comments: My battery is constantly draining, what are some suggestions.\n",
    "```\n",
    "4. Expected format:\n",
    "```\n",
    "Context:<context>\n",
    "Please respond to the following user comment. Use the context above if it is helpful.\n",
    "User comments: <user query>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1732141874258,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "n-J3uoTKegQi",
    "outputId": "55a1e47a-4f2a-4ed0-e7ba-cd166a06205b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: \n",
      "Please respond to the following user comment. Use the context above if it is helpful.\n",
      "User comments: What is the functionality of an LLM?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO 1: Populate the user prompt generating function\n",
    "def prompt_with_context(context, user_query):\n",
    "    # Format the prompt with the provided context and user query\n",
    "    prompt = f\"\"\"\n",
    "Context: {context}\n",
    "Please respond to the following user comment. Use the context above if it is helpful.\n",
    "User comments: {user_query}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "example_prompt = prompt_with_context(\"\", \"What is the functionality of an LLM?\")\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTNVJ2sxYfyb"
   },
   "source": [
    "### Step 2: LLM Query Function\n",
    "\n",
    "In this step we write a function to query the LLM given the context and the user_query, using the prompt generation function that you have implemented above.\n",
    "\n",
    "Recall that LLM works with tokens instead of strings and characters directly, as such we need to tokenize the prompt first using tokenizer and decode the tokens generated by the LLM.\n",
    "\n",
    "**Input:** User query and context\n",
    "\n",
    "**Output:** LLM response as a string\n",
    "\n",
    "Refer to the [Hugging Face Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode_plus) for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3783,
     "status": "ok",
     "timestamp": 1732141878034,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "ag7bW7aMWrIL",
    "outputId": "dfcdeabd-0a75-431e-b48d-7de8fd214089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An LLM (Large Language Model) is designed to understand and generate human-like text based on the input provided. It can be used for various purposes such as language translation, content creation, customer service chatbots, and more. The specific functionalities depend on the model's architecture and training data.\n"
     ]
    }
   ],
   "source": [
    "def get_llm_response(context, user_query, temperature=0.0001):\n",
    "    # Generate the prompt\n",
    "    prompt = prompt_with_context(context, user_query)\n",
    "\n",
    "    # Setting up prompting messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Step 1: Tokenize the input_text\n",
    "    model_inputs = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",  # Ensure tensors are returned for model input\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Step 1.1: Move inputs to the same device as the model\n",
    "    device = model.device  # Automatically detect the model's device\n",
    "    model_inputs = {key: value.to(device) for key, value in model_inputs.items()}  # Move inputs to the model's device\n",
    "\n",
    "    # Step 2: Call the LLM to generate tokenized outputs\n",
    "    generated_token_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    # Step 3: Post-process the generated tokens to remove input tokens\n",
    "    def post_processing(generated_token_ids):\n",
    "        # Remove input tokens from the generated tokens\n",
    "        input_length = model_inputs['input_ids'].shape[1]  # Length of input tokens\n",
    "        return generated_token_ids[:, input_length:]  # Retain only the new tokens\n",
    "\n",
    "    generated_token_ids_post_processed = post_processing(generated_token_ids)\n",
    "\n",
    "    # Step 4: Decode the generated tokens\n",
    "    response = tokenizer.batch_decode(\n",
    "        generated_token_ids_post_processed,\n",
    "        skip_special_tokens=True  # Remove special tokens during decoding\n",
    "    )[0]  # Decode as a string\n",
    "\n",
    "    return response\n",
    "\n",
    "example_prompt_response = get_llm_response(\"\", \"What is the functionality of an LLM?\")\n",
    "print(example_prompt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9az0F5aGg8i"
   },
   "source": [
    "### Step 3: Query the LLM with Different Temperature Settings\n",
    "\n",
    "In this step, we query the LLM using the implemented functions. We are now ready to write the first query to interact with the Qwen 2.5-1.5B LLM.\n",
    "\n",
    "Additionally, we explore the effect of temperature on LLM inference.\n",
    "\n",
    "**Example prompt (empty context):** What is the functionality of an LLM?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4738,
     "status": "ok",
     "timestamp": 1732141882754,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "V56yVZUrJHHV",
    "outputId": "be9252ca-b4d6-46a0-b154-36d87da1d2f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "An LLM (Large Language Model) is designed to understand and generate human-like text based on the input provided. It can be used for various purposes such as language translation, content creation, customer service chatbots, and more. The specific functionalities depend on the model's architecture and training data.\n"
     ]
    }
   ],
   "source": [
    "# TODO 5: uses the get_llm_response function implemented\n",
    "# and print the LLM's response for the example prompt.\n",
    "def query_llm_example():\n",
    "    # Example prompt\n",
    "    context = \"\"  # Empty context\n",
    "    user_query = \"What is the functionality of an LLM?\"\n",
    "    temperature = 0.0001  # Specified temperature\n",
    "\n",
    "    # Query the LLM using the implemented function\n",
    "    response = get_llm_response(context, user_query, temperature)\n",
    "\n",
    "    # Print the LLM's response\n",
    "    print(\"LLM Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Execute the function\n",
    "query_llm_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKjaIAFiKsMW"
   },
   "source": [
    "The temperature parameter in an LLM controls the randomness or creativity of the generated responses. It adjusts the likelihood distribution from which the next token is selected during text generation. If low temperature (close to 0), then the model becomes deterministic, focusing on the most probable tokens. The responses are precise, factual, and consistent, which are suitable for tasks requiring reliability, such as technical explanations or summarizations. However, if high temperature, then the model becomes more random, exploring less probable tokens. The responses are more creative, diverse, and sometimes unpredictable, so they suit tasks like creative writing or brainstorming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26494,
     "status": "ok",
     "timestamp": 1732141909240,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "McIkYlvnKrMH",
    "outputId": "62f15f23-6c29-4293-b8a0-8bd992648ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.0001\n",
      "LLM Response:\n",
      "An LLM (Large Language Model) is designed to understand and generate human-like text based on the input provided. It can be used for various purposes such as language translation, content creation, customer service chatbots, and more. The specific functionalities depend on the model's architecture and training data.\n",
      "--------------------------------------------------\n",
      "Temperature: 0.5\n",
      "LLM Response:\n",
      "An LLM (Large Language Model) is a type of artificial intelligence that can generate human-like text based on the input provided to it. It has the ability to understand and process natural language, allowing it to perform tasks such as translation, summarization, question answering, and more. LLMs are designed to be highly versatile and capable of handling a wide range of inputs and generating appropriate responses. They have been used in various applications, including chatbots, virtual assistants, content generation for websites or social media platforms, and even in fields like medicine and law where they can assist with document analysis and case studies.\n",
      "--------------------------------------------------\n",
      "Temperature: 1.0\n",
      "LLM Response:\n",
      "As an AI language model, my primary function is to generate human-like responses based on the input provided by users. I can answer questions, provide information on various topics, engage in conversations, and assist with tasks such as summarizing text or generating writing prompts. My purpose is to help people by offering useful insights and knowledge that can be applied in different contexts. However, please note that I do not have personal opinions or emotions, but rather aim to provide objective and factual information.\n",
      "--------------------------------------------------\n",
      "Temperature: 1.5\n",
      "LLM Response:\n",
      "LLMs (Large Language Models) are designed to simulate intelligent human-level conversation and problem-solving through intricate algorithms that generate natural language outputs. Their purpose is to provide context-sensitive responses, understanding of context, nuanced reasoning, and the ability to adapt their explanations based on the complexity of the queries or tasks at hand, making them incredibly versatile in a variety of applications ranging from customer service bots, chatbots, personal assistance tools, language translation software to more sophisticated areas such as AI-driven creative content generation for media like articles, stories, music, and movies. LLMs are particularly valuable because they can offer personalized solutions across domains where quick decision-making or complex reasoning capabilities are required but traditional machine learning techniques might not suffice due to the vast volume of data needed for training large models. They leverage unsupervised learning methods that make use of pre-existing internet sources and existing data points rather than requiring extensive labeled examples, which allows these systems to continuously improve over time while being robust enough to operate effectively even with sparse or limited input datasets.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO 6: change the temperature parameter value and see how that affects the LLM output.\n",
    "# You might want to repeatly generate a few response of the same prompt to see the difference.\n",
    "def experiment_with_temperature():\n",
    "    # Example prompt\n",
    "    context = \"\"  # Empty context\n",
    "    user_query = \"What is the functionality of an LLM?\"\n",
    "\n",
    "    # Test different temperature values\n",
    "    for temp in [0.0001, 0.5, 1.0, 1.5]:\n",
    "        print(f\"Temperature: {temp}\")\n",
    "        response = get_llm_response(context, user_query, temperature=temp)\n",
    "        print(\"LLM Response:\")\n",
    "        print(response)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Execute the function\n",
    "experiment_with_temperature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0nkXcBlhrti"
   },
   "source": [
    "### Step 4: Hallucination in LLM\n",
    "LLM can hallucinate and produce factually incorrect or self-contradictory results. From our code output, we found that temperature alone does not directly reduce hallucinations but influences randomness and creativity. Lower temperatures lead to more grounded and consistent responses. In addition, we also discovered that including factual context explicitly guides the model away from potential hallucinations. The response reflects an understanding of the provided context, reducing the risk of irrelevant or unsupported outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8037,
     "status": "ok",
     "timestamp": 1732141917272,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "rcq9XVX5crQX",
    "outputId": "37f6eef3-bfc5-4fd0-dd9a-fc5cf4c5622e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.0001 | Context: \n",
      "LLM Response:\n",
      "I'm sorry, but I don't have any information about when squirrels started using ChatGPT. This question seems to be unrelated to my knowledge base and capabilities as an AI language model. If you have any other questions or need assistance with something else, feel free to ask!\n",
      "--------------------------------------------------\n",
      "Temperature: 0.5 | Context: \n",
      "LLM Response:\n",
      "I'm sorry, but I cannot provide an answer to your question as there is no relevant information provided in the given context. The context does not mention any specific date for when squirrels started using ChatGPT. Please provide more details or clarify your question.\n",
      "--------------------------------------------------\n",
      "Temperature: 1.0 | Context: \n",
      "LLM Response:\n",
      "I'm sorry, but I don't have any information about when squirrels started using ChatGPT. My training data does not include such specific details related to wildlife or squirrel behavior in relation to artificial intelligence systems like ChatGPT. Therefore, I cannot provide an accurate answer based on my current knowledge.\n",
      "--------------------------------------------------\n",
      "Temperature: 0.5 | Context: Squirrels do not use ChatGPT, and there is no record of such an event.\n",
      "LLM Response:\n",
      "The context provided states that squirrels do not use ChatGPT and there is no record of such an event. Therefore, based on this information, we cannot determine the date when squirrels started using ChatGPT. The context does not provide any historical or factual data about squirrel usage of ChatGPT.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO 7: Come up a prompt that results in hallucination, then see\n",
    "# if changing the temperature or providing factual context help?\n",
    "def experiment_with_hallucination():\n",
    "    hallucination_prompt = \"What is the date when squirrels started using ChatGPT?\"\n",
    "\n",
    "    configurations = [\n",
    "        {\"temperature\": 0.0001, \"context\": \"\"},\n",
    "        {\"temperature\": 0.5, \"context\": \"\"},\n",
    "        {\"temperature\": 1.0, \"context\": \"\"},\n",
    "        {\"temperature\": 0.5, \"context\": \"Squirrels do not use ChatGPT, and there is no record of such an event.\"}\n",
    "    ]\n",
    "\n",
    "    for config in configurations:\n",
    "        print(f\"Temperature: {config['temperature']} | Context: {config['context']}\")\n",
    "        response = get_llm_response(config[\"context\"], hallucination_prompt, temperature=config[\"temperature\"])\n",
    "        print(\"LLM Response:\")\n",
    "        print(response)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "experiment_with_hallucination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qKHjsc6cqQA"
   },
   "outputs": [],
   "source": [
    "# Query the LLM with varying temperature settings\n",
    "temperatures = [0.0001, 0.1, 1.0]\n",
    "context = \"\"\n",
    "example_prompt = \"What is the functionality of an LLM?\"\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n--- Response at temperature {temp} ---\")\n",
    "    for _ in range(3):\n",
    "        response = get_llm_response(context, example_prompt, temperature=temp)\n",
    "        print(f\"Response: {response}\")\n",
    "        print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3PdM9shycC3"
   },
   "source": [
    "#### Task 2 - Understanding Tokenization and Embeddings\n",
    "1. Instantiate an open-sourced Hugging Face embedding model.\n",
    "2. Encode the given sentence examples.\n",
    "3. Implement the cosine vector similarity score.\n",
    "4. Compare the embeddings between those examples and a given reference and select the ones that have a similarity score greater than some threshold.\n",
    "\n",
    "### Embedding Model Setup\n",
    "In this project we use the all-mpnet-base-v2 embedding model. all-mpnet-base-v2 is an embedding model based on Microsoft's mpnet-base models. This model is intented to encode sentences and short paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9T7iXsc_1ZK6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.core import Document, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOli1yP39Ttj"
   },
   "outputs": [],
   "source": [
    "# Load a sentence-transformers model for text embeddings from Hugging Face\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vN3k4dDS-aEG"
   },
   "source": [
    "#### Step 1: Embedding Generation\n",
    "\n",
    "In this step we write a funciton that can generate the embeddings used for downstream tasks such as RAG. Populate the embedding generating functions below based on the following specs.\n",
    "\n",
    "**Input:** User input text.\n",
    "\n",
    "**Output:** Text embedding using the embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding shape: torch.Size([768]). This indicates that the embedding is a 1D tensor (a vector) with 768 elements. Each of these 768 values is a numerical feature representing the semantic meaning of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1732141919729,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "ZNOxk-Mz5GD_",
    "outputId": "9f5a81f2-6b5a-4781-b763-e2351d9c706c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# TODO 8: Generate embeddings using the embedding model\n",
    "import torch\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    # Generate embeddings using the encode method of SentenceTransformer\n",
    "    embedding = embed_model.encode(\n",
    "        text,\n",
    "        convert_to_tensor=True,  # Return PyTorch tensor\n",
    "        device=embed_model.device  # Ensure embeddings are computed on the correct device (CPU or GPU)\n",
    "    )\n",
    "    return embedding\n",
    "\n",
    "# Example usage\n",
    "text = \"Sample text for embedding generation.\"\n",
    "embedding = generate_embeddings(text)\n",
    "print(\"Embedding shape:\", embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of the embedding vector remains constant (e.g., 768) regardless of the length of the input sentence. This is because the embedding model (SentenceTransformer) generates fixed-length embeddings based on the model's architecture and not the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1732141919729,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "UWur98IcsmxB",
    "outputId": "daaa3f9e-0c94-4325-90f4-d53cd8a94075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Short sentence.\n",
      "Embedding shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Sentence 2: This is a slightly longer sentence to test embedding length.\n",
      "Embedding shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Sentence 3: An even longer sentence to further examine whether the embedding size remains fixed regardless of input length.\n",
      "Embedding shape: torch.Size([768])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Short sentence.\", \"This is a slightly longer sentence to test embedding length.\", \"An even longer sentence to further examine whether the embedding size remains fixed regardless of input length.\"]\n",
    "embeddings = [generate_embeddings(sentence) for sentence in sentences]\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    print(f\"Sentence {i+1}: {sentences[i]}\")\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yso29AEzCSRW"
   },
   "source": [
    "#### Step 2: Cosine Similarity Score\n",
    "\n",
    "In this step we write a funciton that ccomputes the similarity of two embeddings. We use cosine similarity.\n",
    "\n",
    "**Input:** Two embeddings as numpy arrays.\n",
    "\n",
    "**Output:** The cosine similarity score between the two embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1732141919729,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "eYm4j2uJCmTx",
    "outputId": "10eb50a1-8b2f-4a76-b11c-e104506f36d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO 9: Calculate the cosine similarity score for the two embeddings\n",
    "def cosine_similarity_score(src_embedding, tar_embedding):\n",
    "    # Normalize the embeddings to unit vectors\n",
    "    src_norm = np.linalg.norm(src_embedding)\n",
    "    tar_norm = np.linalg.norm(tar_embedding)\n",
    "\n",
    "    # Handle edge case where embedding norm is zero\n",
    "    if src_norm == 0 or tar_norm == 0:\n",
    "        return 0.0  # No similarity if one of the vectors has zero magnitude\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    score = np.dot(src_embedding, tar_embedding) / (src_norm * tar_norm)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Example embeddings\n",
    "embedding1 = np.array([1, 2, 3])\n",
    "embedding2 = np.array([4, 5, 6])\n",
    "\n",
    "# Compute cosine similarity\n",
    "score = cosine_similarity_score(embedding1, embedding2)\n",
    "print(\"Cosine Similarity Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xptaAlEDJG7"
   },
   "source": [
    "#### Step 3: Comparing Similarity between Sentences.\n",
    "\n",
    "In this step we uses the implemented embedding generation function and the cosine similarity score function to compute the similiarty between pairs of the provided sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 27100,
     "status": "ok",
     "timestamp": 1732141946812,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "_xp5TRIiJzbr",
    "outputId": "4d35a68e-12ef-47a3-84b6-f4004ef4128b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-78dfa835-b9da-4df8-bfe2-a6a95cee3c18\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-78dfa835-b9da-4df8-bfe2-a6a95cee3c18\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sentences.txt to sentences (1).txt\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload sentences.txt\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1732142231524,
     "user": {
      "displayName": "Hsin Chun Cheng",
      "userId": "04026736899018761172"
     },
     "user_tz": 360
    },
    "id": "BOdwM8bG4HJB",
    "outputId": "c7245b3c-4a7a-4326-fda7-664520b8fdef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Sentence: Target Sentence: The weather today is perfect for a walk in the park.\n",
      "Sentence: Jogging in the park is a great way to exercise in the morning.\n",
      "Cosine Similarity Score: 0.4265746772289276\n",
      "--------------------------------------------------\n",
      "Sentence: It might rain later, so the park could get muddy.\n",
      "Cosine Similarity Score: 0.39862650632858276\n",
      "--------------------------------------------------\n",
      "Sentence: It’s an ideal day to take a walk through the park.\n",
      "Cosine Similarity Score: 0.6885090470314026\n",
      "--------------------------------------------------\n",
      "Sentence: A picnic in the park sounds like a fun idea.\n",
      "Cosine Similarity Score: 0.38541653752326965\n",
      "--------------------------------------------------\n",
      "Sentence: The park is a great place to enjoy today’s beautiful weather.\n",
      "Cosine Similarity Score: 0.5804292559623718\n",
      "--------------------------------------------------\n",
      "Sentence: Today’s weather makes it wonderful to walk outdoors in the park.\n",
      "Cosine Similarity Score: 0.6761819124221802\n",
      "--------------------------------------------------\n",
      "Sentence: Walking in the park is delightful on a day like today.\n",
      "Cosine Similarity Score: 0.6319096088409424\n",
      "--------------------------------------------------\n",
      "Sentence: The park is crowded because it’s the weekend.\n",
      "Cosine Similarity Score: 0.28411778807640076\n",
      "--------------------------------------------------\n",
      "Sentence: Parks often close early during the winter season.\n",
      "Cosine Similarity Score: 0.20539377629756927\n",
      "--------------------------------------------------\n",
      "Sentence: The trees in the park are turning vibrant colors this fall.\n",
      "Cosine Similarity Score: 0.18106311559677124\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO 10: Calculate the cosine similarity score between target sentence and\n",
    "# each of the other sentences provided in the sentences.txt file.\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(text):\n",
    "    embedding = embed_model.encode(\n",
    "        text,\n",
    "        convert_to_tensor=True,\n",
    "        device=embed_model.device\n",
    "    )\n",
    "    return embedding.cpu().numpy()\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity_score(src_embedding, tar_embedding):\n",
    "    src_norm = np.linalg.norm(src_embedding)\n",
    "    tar_norm = np.linalg.norm(tar_embedding)\n",
    "    if src_norm == 0 or tar_norm == 0:\n",
    "        return 0.0\n",
    "    return np.dot(src_embedding, tar_embedding) / (src_norm * tar_norm)\n",
    "\n",
    "# Read the file\n",
    "with open(\"sentences.txt\", \"r\") as file:\n",
    "    sentences = file.readlines()\n",
    "\n",
    "# Extract the target sentence and other sentences\n",
    "target_sentence = sentences[0].strip()  # First line is the target sentence\n",
    "other_sentences = [sentence.strip() for sentence in sentences[3:]]\n",
    "\n",
    "# Generate embeddings for the target sentence\n",
    "target_embedding = generate_embeddings(target_sentence)\n",
    "\n",
    "# Calculate and print cosine similarity scores\n",
    "similarity_scores = []\n",
    "for idx, sentence in enumerate(other_sentences):\n",
    "    other_embedding = generate_embeddings(sentence)\n",
    "    score = cosine_similarity_score(target_embedding, other_embedding)\n",
    "    similarity_scores.append((sentence, score))\n",
    "\n",
    "# Display results\n",
    "print(f\"Target Sentence: {target_sentence}\")\n",
    "for sentence, score in similarity_scores:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Cosine Similarity Score: {score}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1jOTcuskjGNkxuMtBBIHg3P5ICC8ToFDj",
     "timestamp": 1732084344216
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
